FORD(unchanged) 83.1711 25.6701 176.735 2449.87
FORD(FARM) 25.6404 7.73062 759.133 5763.26
FORD(FORD_FIXED) 84.8129 27.6884 190.469 2383.36
FORD(FORD_FIXED) 83.8513 27.2989 193.692 2419.29
FORD(FORD) 86.0911 27.0992 158.895 2409.63
FORD(FORD) 85.2736 26.851 159.172 2364.83
FORD(FARM) 25.5982 7.73269 758.061 5757.34
FORD (FORD) 86.3878 27.0716 165.618 2284.87

HALF_TX
FORD (HALF_TX) 95.148 22.487 91.6658 36701.1
(There was an entry here. ignore that on the detailed file)
FORD (FORD_FIXED) 96.7731 22.8942 85.8385 33740.6
FORD (FARM) 23.4556 4.99431 340.778 87877.3


After Enabling caches
FORD(FORD) 85.8425 27.0657 159.647 2457.27
FORD(FARM) 65.6791 21.9873 249.453 3090.74
FORD (FORD FIXED) 84.5225 27.4809 192.623 2308.62

is this becuase of the doorbell batching.?


(test.ignore) FORD 84.8756 27.0825 160.729 2509.74

2 MACHINES, 32 WORKERS, 
FORD 70.0094 18.489 172.503 2088.17
(ignore mistake) FORD 84.3437 27.4668 191.042 2452.81
FORD(FORD_FIXED) 70.727 19.4731 235.48 2113.03
FORD(FARM) 45.7623 13.0431 398.657 10399.4

FORD with new undo logs including inserts
FORD 89.8186 27.756 178.065 2416.31

without new undo logging
FORD 89.996 28.5838 148.964 2255.08

with original parallel undo logs.
FORD 86.9632 27.4307 156.949 2372.18

###############with latch logging###########################################
FORD with original parallel undo logs
FORD 80.4745 25.7363 156.671 3763.81

FORD with no undo logging
FORD 82.909 26.8369 149.305 4587.77

FORD with new undo logs including inserts
FORD 85.5079 26.9874 162.017 3886.72

WIthout logging
16 threads
FORD 90.623 28.9649 149.034 2169.01
32 threads
FORD 65.5874 16.8633 414.153 3378.56
64
FORD 71.2348 16.5828 550.036 10043
8 threads????
FORD 75.368 30.7891 89.1424 1849.98

1 threads 8 corutines
FORD 14.6741 12.1378 83.237 1947.38

1 thread 16 coroutines
FORD 16.9459 11.6083 103.781 3350.78

1 thread 16 coro
FORD 16.9161 11.5513 103.615 3366.06
1 thread 4 coros
FORD 9.21799 8.50157 137.399 3354.88



with latch logging

1 thread 4 coros
FORD 7.67464 7.01846 98.439 3281.17

1 thread 2 coros(smallest)
FORD 3.16233 3.16233 199.202 2691.5

this is withou logging for 1 thread and 2 coroutines. 
FORD 3.16286 3.16286 199.114 2694.09

1thread 8 coros without latch logging
FORD 11.4345 9.41993 128.159 3153.86

1 thread 8 coros with latch logging
FORD 12.676 10.0896 77.112 4096.74


with 3 way replication, three memory machines. 1 coordinator
FORD 83.069 26.9456 158.476 2134.47
FORD 81.8893 26.4732 160.427 2143.05


FORD 75.8448 23.9428 183.558 3119.62


FORD 70.2889 22.8205 179.22 3589.89
FORD 80.6333 26.328 160.931 2195.65

My assumption was right. single QP makes a huge overhead on the tx running on a single thread. no wonder why ford makes huge gains from batching just read+cas. 

lets take.
1thread 1 coro
FORD 4.02621 4.02621 137.576 2668.39
FORD 14.6627 12.153 80.488 1970
FORD 56.829 32.2487 48.4036 1523.79
FORD 80.6459 26.2929 162.296 2209.11

with latch logging
FORD 3.2657 3.2657 202.333 2673.4
FORD 12.7093 10.0624 78.119 2446.3
FORD 48.298 27.3521 53.2134 1668.47
FORD 72.2233 23.4704 172.895 3588.2


3 way replication with 3 coordinators.
16 threads 8 coros
FORD 39.0544 9.37547 334.071 3508.07

avoid.
//FORD 15.1293 12.5391 76.783 1871.56

1 thread 8 coros per machine
FORD 12.7236 7.57996 73.638 1600.49

//FORD 52.0547 28.9143 60.5782 1460.3

4 threads 8 coros
FORD 30.5504 11.8736 97.8237 1792.04

I have kept the replication degree the same way
lets change the replication degree to 2 and see.


//FORD 51.8763 28.8777 60.9087 1484.58
4 threads, 8 coros.
FORD 29.9627 11.5163 101.03 1820.05

1 thread 8 coro.
FORD 15.9391 10.1 55.575 1030.19


bandwidth tests
1thread 8 coros
FORD 15.9039 10.564 59.531 1050.68
NW Counters: | port_xmit_data: 1.62 Gbits || port_rcv_data: 15.40 Gbits || port_xmit_packets: 1.18 MPS || port_rcv_packets: 1.41 MPS 

4 thread 8 coros
FORD 30.5463 11.5312 97.457 1684.89
NW Counters: | port_xmit_data: 2.11 Gbits || port_rcv_data: 31.64 Gbits || port_xmit_packets: 2.14 MPS || port_rcv_packets: 2.48 MPS |


one coordinator and 3 machines with 3 replicas. 
1 thread 8 corors
FORD 15.15 12.5454 77.002 1926.99
NW Counters: | port_xmit_data: 1.70 Gbits || port_rcv_data: 11.22 Gbits || port_xmit_packets: 1.12 MPS || port_rcv_packets: 1.27 MPS |

4 thread 8 coros
FORD 52.0159 29.0099 60.8043 1463.98
NW Counters: | port_xmit_data: 4.76 Gbits || port_rcv_data: 59.51 Gbits || port_xmit_packets: 3.94 MPS || port_rcv_packets: 4.76 MPS |

8 thread 8 coros.
FORD 69.5529 29.4401 92.8277 1754.58
NW Counters: | port_xmit_data: 4.31 Gbits || port_rcv_data: 73.96 Gbits || port_xmit_packets: 4.32 MPS || port_rcv_packets: 5.37 MPS |

10 threads 8 coros. 
FORD 72.6883 27.9227 112.35 1924.06
NW Counters: | port_xmit_data: 5.03 Gbits || port_rcv_data: 84.64 Gbits || port_xmit_packets: 5.32 MPS || port_rcv_packets: 6.48 MPS |


checking memory side bandwidth with node-2(memory-0)
FORD 72.7448 27.9186 113.103 1904.56
single coordinator here
NW Counters: | port_xmit_data: 5.02 Gbits || port_rcv_data: 84.86 Gbits || port_xmit_packets: 5.31 MPS || port_rcv_packets: 6.48 MPS 
memory side (with 3 machines, 3 way replication it saturates)
NW Counters: | port_xmit_data: 80.84 Gbits || port_rcv_data: 3.51 Gbits || port_xmit_packets: 5.23 MPS || port_rcv_packets: 4.04 MPS |


FORD 72.6943 28.0287 111.32 1986.05
memory node-2 (node 1)
NW Counters: | port_xmit_data: 0.64 Gbits || port_rcv_data: 0.92 Gbits || port_xmit_packets: 0.44 MPS || port_rcv_packets: 0.46 MPS |


FORD 72.6421 27.8389 113.099 1889.68
memory node 3 (memory node -2)
NW Counters: | port_xmit_data: 3.27 Gbits || port_rcv_data: 0.71 Gbits || port_xmit_packets: 0.71 MPS || port_rcv_packets: 0.68 MPS |
cnclustion: some tables are highly congested. almost all the accesses are directed to the memory node -0. thats why increasing number of machines won't increase the throughput.

with lathc logging.
FORD 71.7529 26.6695 108.148 5233.23
compute : NW Counters: | port_xmit_data: 4.53 Gbits || port_rcv_data: 65.50 Gbits || port_xmit_packets: 4.88 MPS || port_rcv_packets: 5.78 MPS |
memory 0: NW Counters: | port_xmit_data: 54.03 Gbits || port_rcv_data: 2.53 Gbits || port_xmit_packets: 3.72 MPS || port_rcv_packets: 2.93 MPS |
memory 1: NW Counters: | port_xmit_data: 0.90 Gbits || port_rcv_data: 1.21 Gbits || port_xmit_packets: 0.87 MPS || port_rcv_packets: 0.88 MPS |
memory 2: NW Counters: | port_xmit_data: 2.96 Gbits || port_rcv_data: 1.28 Gbits || port_xmit_packets: 1.22 MPS || port_rcv_packets: 1.21 MPS |

latch logging does not show significant impact in tpcc why?.

